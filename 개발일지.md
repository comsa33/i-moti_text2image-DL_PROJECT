# 프로젝트 text2img 개발 일지

텍스트를 입력으로 받아 텍스트 의미에 해당하는 이모티콘을 생성해내는 모델 개발

# 개발 1일차

## 모델 구상 : Lstm + GAN

### Generator

- 텍스트 → 벡터화 → 모델 입력
- 임베딩 후 LSTM or GRU layer 통과
- reshape → upsampling (72, 72, 4) - sigmoid

### Discriminator

## 실험 결과

- 10000 epochs 까지 훈련 시 동일한 노이즈만 출력
- 층 노드 수를 튜닝 후 동일한 결과 출력

## 소감

- 너무 쉽게 생각했던 것 같다.
- 기존 image to text DL 모델들의 구조를 살펴보니 접근방식이 완전히 달랐다.
- 이 분야에서 가장 최신 논문인 Dall-e 를 확인해보면 d-vae 를 사용하여 이미지의 중요특성을 뽑아 낸 후 이미지 토큰을 만들어 텍스트와 concat 하여 transformer 를 사용하는 구조를 가지고 있다.
- 이모티콘 이미지는 고화질의 큰 이미지가 아니므로 Dall-e 보다는 좀 더 간소화된 형태로 구현해 낼 수 있지 않을까 하는 가설

# 개발 2일차

## 모델 구상 : VAE + padded word token

- 단순히 글자의 특징을 통해 4채널의 RGB 배열을 만들 수 있다면 가장 쉽고 효율적인 모델이겠지만 제대로된 결과를 출력하지 못했다.
    - 실패요인 : 아마도 상대적으로 적은 텍스트 벡터 자체만으로 타겟 이미지의 수많은 중요 특징을 뽑아낸다는 것이 무리수 였던것 같다. 특히나 데이터 하나의 텍스트 길이가 1~6 정도로 매우 짧은 반면에 이미지의 벡터화 된 배열은 (72*72*4) 이기 때문에 중요한 특징을 뽑아내기에 적합하지 않은 데이터 라고 생각한다.
- 개선방법
    - 첫번째 방법에서 부족한 부분이라고 예측했던 이미지의 주요특징을 따로 뽑은 다음 텍스트 벡터를 함께 입력으로 해당 이미지를 재생성하는 훈련을 한다. 그리고 텍스트만 입력으로 이미지를 생성하는 테스트를 실험해보고자 한다.
- VAE + text vector 모델 스케치
    - 인코더 (훈련 시에만 사용)
        - 원본 이미지를 입력 받아 압축하여 latent vector 를 추출
    - 디코더
        - 인코더를 통해 나온 latent vector + text vector 를 입력
        - 원본 이미지와 동일한 크기로 이미지 재생성

## 실험 결과

- 100 epochs까지 훈련 후 테스트 시 알 수 없는 노이즈 이미지만 출력
- 10000 epochs 훈련 후 디코더를 통해 텍스트만 입력 했을 시 얼추 알아볼 수 있는 저화질의 이모티콘 이미지 생성
- 위 방법으로 텍스트를 통해 이미지를 생성해내는 것이 꽤나 성과가 있었던 실험이었다.
![text2img_1](https://user-images.githubusercontent.com/61719257/154234773-d459b249-3e8a-44d9-a71b-295b7771ac44.png)


## 실험 시 문제점

- 텐서 타입 호환 문제 → 해당 문제 발생 지점에 문제가 되는 텐서를 tf.cast 를 통해 타입 변경
- 텍스트 벡터를 인코더와 디코더 사이에 넣었을 경우 디코더에서 가중치 초기화 문제 발생 → 텐서플로우 2.0 으로 들어오면서 고차원 api를 사용함으로 인해 생기는 충돌로 보임 → 문제가 일어나는 해당 함수에 ‘@tf.function’ 으로 랩핑해주면 해결 ([ValueError: Creating variables on a non-first call to a function decorated with tf.function.](https://www.notion.so/TF-ValueError-Creating-variables-on-a-non-first-call-to-a-function-decorated-with-tf-function-8aa96453f90b48deaa5807d84f070113))

## 소감

- 주말 포함해서 읽은 DALL-e 논문과 고민에 고민을 거듭한 보람이 있는 것 같다.
- 아직 100개의 구글 이모티콘으로만 실험을 했기 때문에 더 많은 데이터로 훈련을 할 시 좀 더 나은 성능과 결과물을 생성해 낼지도 모른다는 기대감이 있다.
- 모델을 다루는데 가장 중요한 것 중 하나는 분명히 텐서를 잘 다루는 일임에 분명하다.

# 개발 3일차

## 모델 구상 : C-GAN

- 2일차에 사용한 VAE 모델의 Decoder에 Word vector 를 concat 한 예측 결과는 심각한 문제점이 있었다.
    - 테스트 시 입력한 텍스트의 문맥을 전혀 고려하지 않고 랜덤한 몇몇 이미지만 반복적으로 출력하는 것으로 보인다.
- 그래서 이번에는 GAN 모델에 control 이 가능한 Conditional GAN 모델을 통해 실험을 해보기로 했다.
- 모델의 base structure 는 [keras 공식문서 conditional GAN example](https://keras.io/examples/generative/conditional_gan/) 을 사용했다.
- generator 와 discriminator 의 입력에 text를 label 로써 추가정보를 함께 넣어서 이미지를 생성 판단하도록 하였다.

## 실험 결과

- 추상적인 이미지가 생성이 되고는 있지만 정확한 형태를 알아보기 어렵다.
![animation_0 001-e5](https://user-images.githubusercontent.com/61719257/154234837-8f4f13c5-d003-4fed-99c7-ea2150ed32d6.gif)
![animation_0 001-e17](https://user-images.githubusercontent.com/61719257/154234845-b1fddc96-26ff-4294-a65d-58f3d01ce87b.gif)
![animation_0 001-e25](https://user-images.githubusercontent.com/61719257/154234862-19a259bd-d5e0-4c9d-8885-18705e63c66e.gif)
![animation_0 001-e30](https://user-images.githubusercontent.com/61719257/154234876-bbb2c9da-9b24-4586-a5d5-e9d7ddc7971b.gif)
![animation_0 001-f4-e12](https://user-images.githubusercontent.com/61719257/154234890-767f899e-1f1e-414d-ac8b-c321151076c3.gif)
![animation_0 001-f4](https://user-images.githubusercontent.com/61719257/154234909-836b4904-4ccc-4432-8f21-c5bbe88fc931.gif)
![animation_0 001-f6](https://user-images.githubusercontent.com/61719257/154234926-8de4d7f4-54cc-4f2c-bd37-3fbfd4fd8a98.gif)
![animation_0 01](https://user-images.githubusercontent.com/61719257/154234942-58fb6f50-d795-4881-b219-b1e0f6da272f.gif)
![animation_0 001](https://user-images.githubusercontent.com/61719257/154234964-951ae7ae-70ec-4584-87d3-1889c17689e8.gif)


# 개발 4일차

## 모델 구상 : VAE + word embedding layer

- stackGAN 논문을 읽다가 stackGAN 에서 자신들이 만든 특별한 embedding vector를 사용했다는 것을 알게되었다. 혹시 지금까지 내가 만든 모델들이 문맥파악에 실패한 이유가 word embedding 과정 없이 padded vector만을 그대로 이미지 vector에 concat 한 것 때문이 아닐까 하는 생각이 불현듯 스쳐지나갔다.
- C-GAN 에 word embedding vector 를 사용하는 것이 stack GAN 의 stage1 이라면 VAE 에 word embedding vector 를 추가하면 비슷한 결과를 얻게 되지 않을까 생각했다.
- encoder 와 decoder 사이에 embedding layer 를 구축하고 decoder 입력 이전 encoder에서 나온 latent vector 와 concat 하였음

## 실험 결과

- 확실히 문맥을 이해하는 듯 하다.
    - ‘smiling’ 이 들어간 텍스트에 웃는 얼굴 이모티콘이 ‘cold’ 가 들어간 텍스트에는 추워하는 이모티콘이 생성이 되고 있다.
- 지금까지 나온 결과 중 가장 그럴 듯 해보이는 결과이지만 여전히 한계점이 있다.
    - 예를 들어, ‘웃는 얼굴에 화난 눈’ 혹은 ‘토하는 똥’ 과 같이 기존에 없는 이모티콘을 생성하기 위해 테스트를 해본 결과 자연스러운 이미지 합성은 불가능으로 보인다.
    - 창의적인 이모티콘이라고 볼 수 없을 정도로 기존 이모티콘의 형태와 비슷한 결과를 만들어낸다.
    ![download-3](https://user-images.githubusercontent.com/61719257/154234575-e1aaf318-edbf-41f1-b368-48e17c8642a6.png)

    
