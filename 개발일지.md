# 프로젝트 text2img 개발 일지

텍스트를 입력으로 받아 텍스트 의미에 해당하는 이모티콘을 생성해내는 모델 개발

# 개발 1일차

## 모델 구상 : Lstm + GAN

### Generator

- 텍스트 → 벡터화 → 모델 입력
- 임베딩 후 LSTM or GRU layer 통과
- reshape → upsampling (72, 72, 4) - sigmoid

### Discriminator

## 실험 결과

- 10000 epochs 까지 훈련 시 동일한 노이즈만 출력
- 층 노드 수를 튜닝 후 동일한 결과 출력

## 소감

- 너무 쉽게 생각했던 것 같다.
- 기존 image to text DL 모델들의 구조를 살펴보니 접근방식이 완전히 달랐다.
- 이 분야에서 가장 최신 논문인 Dall-e 를 확인해보면 d-vae 를 사용하여 이미지의 중요특성을 뽑아 낸 후 이미지 토큰을 만들어 텍스트와 concat 하여 transformer 를 사용하는 구조를 가지고 있다.
- 이모티콘 이미지는 고화질의 큰 이미지가 아니므로 Dall-e 보다는 좀 더 간소화된 형태로 구현해 낼 수 있지 않을까 하는 가설

# 개발 2일차

## 모델 구상 : VAE + padded word token

- 단순히 글자의 특징을 통해 4채널의 RGB 배열을 만들 수 있다면 가장 쉽고 효율적인 모델이겠지만 제대로된 결과를 출력하지 못했다.
    - 실패요인 : 아마도 상대적으로 적은 텍스트 벡터 자체만으로 타겟 이미지의 수많은 중요 특징을 뽑아낸다는 것이 무리수 였던것 같다. 특히나 데이터 하나의 텍스트 길이가 1~6 정도로 매우 짧은 반면에 이미지의 벡터화 된 배열은 (72*72*4) 이기 때문에 중요한 특징을 뽑아내기에 적합하지 않은 데이터 라고 생각한다.
- 개선방법
    - 첫번째 방법에서 부족한 부분이라고 예측했던 이미지의 주요특징을 따로 뽑은 다음 텍스트 벡터를 함께 입력으로 해당 이미지를 재생성하는 훈련을 한다. 그리고 텍스트만 입력으로 이미지를 생성하는 테스트를 실험해보고자 한다.
- VAE + text vector 모델 스케치
    - 인코더 (훈련 시에만 사용)
        - 원본 이미지를 입력 받아 압축하여 latent vector 를 추출
    - 디코더
        - 인코더를 통해 나온 latent vector + text vector 를 입력
        - 원본 이미지와 동일한 크기로 이미지 재생성

## 실험 결과

- 100 epochs까지 훈련 후 테스트 시 알 수 없는 노이즈 이미지만 출력
- 10000 epochs 훈련 후 디코더를 통해 텍스트만 입력 했을 시 얼추 알아볼 수 있는 저화질의 이모티콘 이미지 생성
- 위 방법으로 텍스트를 통해 이미지를 생성해내는 것이 꽤나 성과가 있었던 실험이었다.

## 실험 시 문제점

- 텐서 타입 호환 문제 → 해당 문제 발생 지점에 문제가 되는 텐서를 tf.cast 를 통해 타입 변경
- 텍스트 벡터를 인코더와 디코더 사이에 넣었을 경우 디코더에서 가중치 초기화 문제 발생 → 텐서플로우 2.0 으로 들어오면서 고차원 api를 사용함으로 인해 생기는 충돌로 보임 → 문제가 일어나는 해당 함수에 ‘@tf.function’ 으로 랩핑해주면 해결 ([ValueError: Creating variables on a non-first call to a function decorated with tf.function.](https://www.notion.so/TF-ValueError-Creating-variables-on-a-non-first-call-to-a-function-decorated-with-tf-function-8aa96453f90b48deaa5807d84f070113))

## 소감

- 주말 포함해서 읽은 DALL-e 논문과 고민에 고민을 거듭한 보람이 있는 것 같다.
- 아직 100개의 구글 이모티콘으로만 실험을 했기 때문에 더 많은 데이터로 훈련을 할 시 좀 더 나은 성능과 결과물을 생성해 낼지도 모른다는 기대감이 있다.
- 모델을 다루는데 가장 중요한 것 중 하나는 분명히 텐서를 잘 다루는 일임에 분명하다.