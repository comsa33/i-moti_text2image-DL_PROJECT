# 프로젝트 text2img 개발 일지

텍스트를 입력으로 받아 텍스트 의미에 해당하는 이모티콘을 생성해내는 모델 개발

# 개발 1일차

## 모델 구상 : Lstm + GAN

### Generator

- 텍스트 → 벡터화 → 모델 입력
- 임베딩 후 LSTM or GRU layer 통과
- reshape → upsampling (72, 72, 4) - sigmoid

### Discriminator

## 실험 결과

- 10000 epochs 까지 훈련 시 동일한 노이즈만 출력
- 층 노드 수를 튜닝 후 동일한 결과 출력

## 소감

- 너무 쉽게 생각했던 것 같다.
- 기존 image to text DL 모델들의 구조를 살펴보니 접근방식이 완전히 달랐다.
- 이 분야에서 가장 최신 논문인 Dall-e 를 확인해보면 d-vae 를 사용하여 이미지의 중요특성을 뽑아 낸 후 이미지 토큰을 만들어 텍스트와 concat 하여 transformer 를 사용하는 구조를 가지고 있다.
- 이모티콘 이미지는 고화질의 큰 이미지가 아니므로 Dall-e 보다는 좀 더 간소화된 형태로 구현해 낼 수 있지 않을까 하는 가설

# 개발 2일차

## 모델 구상 : VAE + padded word token

- 단순히 글자의 특징을 통해 4채널의 RGB 배열을 만들 수 있다면 가장 쉽고 효율적인 모델이겠지만 제대로된 결과를 출력하지 못했다.
    - 실패요인 : 아마도 상대적으로 적은 텍스트 벡터 자체만으로 타겟 이미지의 수많은 중요 특징을 뽑아낸다는 것이 무리수 였던것 같다. 특히나 데이터 하나의 텍스트 길이가 1~6 정도로 매우 짧은 반면에 이미지의 벡터화 된 배열은 (72*72*4) 이기 때문에 중요한 특징을 뽑아내기에 적합하지 않은 데이터 라고 생각한다.
- 개선방법
    - 첫번째 방법에서 부족한 부분이라고 예측했던 이미지의 주요특징을 따로 뽑은 다음 텍스트 벡터를 함께 입력으로 해당 이미지를 재생성하는 훈련을 한다. 그리고 텍스트만 입력으로 이미지를 생성하는 테스트를 실험해보고자 한다.
- VAE + text vector 모델 스케치
    - 인코더 (훈련 시에만 사용)
        - 원본 이미지를 입력 받아 압축하여 latent vector 를 추출
    - 디코더
        - 인코더를 통해 나온 latent vector + text vector 를 입력
        - 원본 이미지와 동일한 크기로 이미지 재생성

## 실험 결과

- 100 epochs까지 훈련 후 테스트 시 알 수 없는 노이즈 이미지만 출력
- 10000 epochs 훈련 후 디코더를 통해 텍스트만 입력 했을 시 얼추 알아볼 수 있는 저화질의 이모티콘 이미지 생성
- 위 방법으로 텍스트를 통해 이미지를 생성해내는 것이 꽤나 성과가 있었던 실험이었다.
![text2img_1](https://user-images.githubusercontent.com/61719257/154234773-d459b249-3e8a-44d9-a71b-295b7771ac44.png)


## 실험 시 문제점

- 텐서 타입 호환 문제 → 해당 문제 발생 지점에 문제가 되는 텐서를 tf.cast 를 통해 타입 변경
- 텍스트 벡터를 인코더와 디코더 사이에 넣었을 경우 디코더에서 가중치 초기화 문제 발생 → 텐서플로우 2.0 으로 들어오면서 고차원 api를 사용함으로 인해 생기는 충돌로 보임 → 문제가 일어나는 해당 함수에 ‘@tf.function’ 으로 랩핑해주면 해결 ([ValueError: Creating variables on a non-first call to a function decorated with tf.function.](https://www.notion.so/TF-ValueError-Creating-variables-on-a-non-first-call-to-a-function-decorated-with-tf-function-8aa96453f90b48deaa5807d84f070113))

## 소감

- 주말 포함해서 읽은 DALL-e 논문과 고민에 고민을 거듭한 보람이 있는 것 같다.
- 아직 100개의 구글 이모티콘으로만 실험을 했기 때문에 더 많은 데이터로 훈련을 할 시 좀 더 나은 성능과 결과물을 생성해 낼지도 모른다는 기대감이 있다.
- 모델을 다루는데 가장 중요한 것 중 하나는 분명히 텐서를 잘 다루는 일임에 분명하다.

# 개발 3일차

## 모델 구상 : C-GAN

- 2일차에 사용한 VAE 모델의 Decoder에 Word vector 를 concat 한 예측 결과는 심각한 문제점이 있었다.
    - 테스트 시 입력한 텍스트의 문맥을 전혀 고려하지 않고 랜덤한 몇몇 이미지만 반복적으로 출력하는 것으로 보인다.
- 그래서 이번에는 GAN 모델에 control 이 가능한 Conditional GAN 모델을 통해 실험을 해보기로 했다.
- 모델의 base structure 는 [keras 공식문서 conditional GAN example](https://keras.io/examples/generative/conditional_gan/) 을 사용했다.
- generator 와 discriminator 의 입력에 text를 label 로써 추가정보를 함께 넣어서 이미지를 생성 판단하도록 하였다.

## 실험 결과

- 추상적인 이미지가 생성이 되고는 있지만 정확한 형태를 알아보기 어렵다.

![animation_0 001-e5](https://user-images.githubusercontent.com/61719257/154234837-8f4f13c5-d003-4fed-99c7-ea2150ed32d6.gif)
![animation_0 001-e17](https://user-images.githubusercontent.com/61719257/154234845-b1fddc96-26ff-4294-a65d-58f3d01ce87b.gif)
![animation_0 001-e25](https://user-images.githubusercontent.com/61719257/154234862-19a259bd-d5e0-4c9d-8885-18705e63c66e.gif)
![animation_0 001-e30](https://user-images.githubusercontent.com/61719257/154234876-bbb2c9da-9b24-4586-a5d5-e9d7ddc7971b.gif)
![animation_0 001-f4-e12](https://user-images.githubusercontent.com/61719257/154234890-767f899e-1f1e-414d-ac8b-c321151076c3.gif)
![animation_0 001-f4](https://user-images.githubusercontent.com/61719257/154234909-836b4904-4ccc-4432-8f21-c5bbe88fc931.gif)
![animation_0 001-f6](https://user-images.githubusercontent.com/61719257/154234926-8de4d7f4-54cc-4f2c-bd37-3fbfd4fd8a98.gif)
![animation_0 01](https://user-images.githubusercontent.com/61719257/154234942-58fb6f50-d795-4881-b219-b1e0f6da272f.gif)
![animation_0 001](https://user-images.githubusercontent.com/61719257/154234964-951ae7ae-70ec-4584-87d3-1889c17689e8.gif)


# 개발 4일차

## 모델 구상 : VAE + word embedding layer

- stackGAN 논문을 읽다가 stackGAN 에서 자신들이 만든 특별한 embedding vector를 사용했다는 것을 알게되었다. 혹시 지금까지 내가 만든 모델들이 문맥파악에 실패한 이유가 word embedding 과정 없이 padded vector만을 그대로 이미지 vector에 concat 한 것 때문이 아닐까 하는 생각이 불현듯 스쳐지나갔다.
- C-GAN 에 word embedding vector 를 사용하는 것이 stack GAN 의 stage1 이라면 VAE 에 word embedding vector 를 추가하면 비슷한 결과를 얻게 되지 않을까 생각했다.
- encoder 와 decoder 사이에 embedding layer 를 구축하고 decoder 입력 이전 encoder에서 나온 latent vector 와 concat 하였음

## 실험 결과

- 확실히 문맥을 이해하는 듯 하다.
    - ‘smiling’ 이 들어간 텍스트에 웃는 얼굴 이모티콘이 ‘cold’ 가 들어간 텍스트에는 추워하는 이모티콘이 생성이 되고 있다.
- 지금까지 나온 결과 중 가장 그럴 듯 해보이는 결과이지만 여전히 한계점이 있다.
    - 예를 들어, ‘웃는 얼굴에 화난 눈’ 혹은 ‘토하는 똥’ 과 같이 기존에 없는 이모티콘을 생성하기 위해 테스트를 해본 결과 자연스러운 이미지 합성은 불가능으로 보인다.
    - 창의적인 이모티콘이라고 볼 수 없을 정도로 기존 이모티콘의 형태와 비슷한 결과를 만들어낸다.
    ![download-3](https://user-images.githubusercontent.com/61719257/154234575-e1aaf318-edbf-41f1-b368-48e17c8642a6.png)

# 개발 5일차 (2022-02-17)

## 모델 구상

### 1. C-GAN + word embedding layer

- 창의적인 이미지 생성이 어렵다는 부분은 VAE 의 단점으로 보인다.
- 그래서 앞서 실험했던 C-GAN 에 word embedding layer 를 추가하여 실험을 해보았다.
    
    ### 실험 결과
    
    > **100 epochs**
    > 
    > ![animation](https://user-images.githubusercontent.com/61719257/154680274-39806b3b-cec4-491f-bf6c-0483387b0e45.gif)
    > 
    > 1. padded vector 를 추가했을 때보다는 확실히 진전이 있어보인다.
    > 2. 가장 큰 문제는 d-loss 는 훈련이 진행됨에 따라 줄어드는 반면에 g-loss는 급격한 증가현상을 보인다.
    > 3. 2번 문제는 learning rate 를 0.001 → 0.0001 로 조절하고 embedding layer 이후 GRU layer를 통과시킨 결과를 generator 입력 노이즈에 concat 을 해줌으로써 조금 완화된 효과를 볼 수 있었다.
    > 4. 하지만 여전히 단순한 GAN loss 계산하는 방식이 마음에 걸려 이 보다 더 진화된 방식이 없는지 고민을 했다.
    > 5. 그래서 C-GAN 과 비슷한 구조를 갖고 있는 Stack GAN 을 사용해보기로 했다.
    

### 2. StackGAN : stage 1

- **char-CNN-RNN**
    
    대충 구조는 텍스트를 conv층을 거치고 난 뒤 RNN 셀을 통과하여 Linear 층을 최종적으로 거쳐 나오는 형태인데 아무리 구글링을 해봐도 정확히 이 구조가 입력된 데이터를 어떻게 해서 무슨 효과를 기대하는 건지를 알 수가 없다...
    
    ### **실험결과**
    
    > **574 epochs**
    > 
    > ![gen_574_9](https://user-images.githubusercontent.com/61719257/154680224-53c3e746-394e-4ea9-ba5c-e64b8e936778.png)
    > 
    > 1. generator 에서 생성하는 이미지가 너무나 무의미한 결과를 만들어낸다.
    > 2. 아무래도 stage1 이라 그런가라고 치부하기에는 이모티콘의 흔적이 조금도 보이지 않는다.
    > 3. 어느 부분에서 훈련이 제대로 진행되지 않는지 연구가 필요해보인다.

# 개발 6일차 (2022-02-18)

## 모델 구상

- reference
    
    [https://github.com/martinduartemore/char_cnn_rnn_pytorch](https://github.com/martinduartemore/char_cnn_rnn_pytorch)
    

### C-GAN + StackGAN + char-cnn-rnn word embedding layer

> 아무리 찾아봐도 tensorflow 기반 char-cnn-rnn 은 나오지 않고, 자세한 모델에 대한 설명도 부족하여 [pytorch 기반의](https://github.com/martinduartemore/char_cnn_rnn_pytorch)  코드를 참고하여 모델의 핵심 부분만을 재구현해서 word embedding vector 를 1024 크기로 반환하도록 했다.
> 
> 
> ```python
> self.word_embedding = keras.Sequential(
>         [
>             keras.layers.InputLayer(input_shape=(self.len_text,)),
>             layers.Embedding(vocab_size, 300, input_length=self.len_text),
>             layers.Conv1D(256, 7, padding="same", strides=3),
>             layers.BatchNormalization(),
>             layers.LeakyReLU(alpha=0.2),
>             layers.MaxPool1D(pool_size=2, strides=2),
>             layers.Conv1D(128, 7, padding="same", strides=3),
>             layers.BatchNormalization(),
>             layers.LeakyReLU(alpha=0.2),
>             layers.MaxPool1D(pool_size=2, strides=2),
>             layers.GRU(128),
>             layers.Flatten(),
>             layers.Dense(num_classes),
>         ])
> ```
> 
- 184 epoch 지점에서 훈련이 멈추어 진행이 되지 않는 상황이 발생하여 (아마 resource 부족) 은닉층의 파라미터를 좀 더 낮게 설정하여 해결함
- 현재 2000 epochs 까지 훈련을 진행중

### 실험결과 (500 epochs 전)
![c-gan-42-256-1645151691](https://user-images.githubusercontent.com/61719257/154680394-09ffc762-8b0a-402b-bd00-08a85e8226f9.gif)
![c-gan-42-256-1645155585](https://user-images.githubusercontent.com/61719257/154680406-d174f298-67bc-4aa5-9008-e9159d63dd31.gif)
![c-gan-42-256-1645167455](https://user-images.githubusercontent.com/61719257/154680422-faf1936d-d50a-4486-a81f-6c357e29c89c.gif)
![c-gan-42-256-1645185747](https://user-images.githubusercontent.com/61719257/154680442-b1798301-5344-4fa4-b164-a477c0eb86d1.gif)

# 개발 7일차 (2022-02-19)

## 모델 구상

- 좀처럼 이미지 생성에서 개선이 보이지 않는다.
- character level word embedding 대신 단어 기반으로 다시 바꿔서 진행을 함


![download-1](https://user-images.githubusercontent.com/61719257/155095137-324f6434-b80e-4ba1-867e-2f83396e9c53.png)![download-1](https://user-images.githubusercontent.com/61719257/155095145-b2137068-572a-47e9-8449-a09c6e76eeea.png)![download](https://user-images.githubusercontent.com/61719257/155095190-1f2b4847-db8c-45f5-abdb-79d1e1584b30.png)

# 개발 8-10일차 (2022-02-20 - 2022-02-22)

### **진행 내용**

- bert 로 텍스트 토큰과 이미지 토큰을 입력으로 이미지 토큰을 생성해낼 수 있는 방법을 고민. 
- seq to seq  형태로 시도를 해보았지만 코랩프로와 로컬 모두 메모리 부족으로 커널 종료.
- 결국 100개의 데이터까지만 테스트 가능.
- 모델의 마지막 레이어를 통과한 벡터를 사용해보기로 함
- Dall-e 모델 구조는 리소스 부족으로 테스트가 힘들것 같아서 기존에 구성한 C-GAN+StackGAN 구조의 word embedding vector 자리에 text-image-seq2seq-output vector로 대체해보았음
- 파이프라인 내에서 텐서 구조가 충돌이 있는 듯함.
- 모델 전체 구조 수정이 필요해 보임


